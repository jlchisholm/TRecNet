Model Name: JetPretrainer 
Model ID: JetPretrainer_6jets_20230530_112415 

 ---------------------------------------------------  
Training Data File: /mnt/xrootdg/jchishol/mntuples_08_01_22/variables_ttbar_ljets_10j_train.h5 
X Maxmean File: /mnt/xrootdg/jchishol/mntuples_08_01_22/X_maxmean_variables_ttbar_ljets_10j_train.npy 
Y Maxmean File: /mnt/xrootdg/jchishol/mntuples_08_01_22/Y_maxmean_variables_ttbar_ljets_10j_train.npy 

 ---------------------------------------------------  
X Keys: j1_pt, j1_eta, j1_phi, j1_m, j1_isbtag, j2_pt, j2_eta, j2_phi, j2_m, j2_isbtag, j3_pt, j3_eta, j3_phi, j3_m, j3_isbtag, j4_pt, j4_eta, j4_phi, j4_m, j4_isbtag, j5_pt, j5_eta, j5_phi, j5_m, j5_isbtag, j6_pt, j6_eta, j6_phi, j6_m, j6_isbtag, lep_pt, lep_eta, lep_phi, met_met, met_phi
X Scaled Keys: j1_pt, j1_px, j1_py, j1_eta, j1_m, j1_isbtag, j2_pt, j2_px, j2_py, j2_eta, j2_m, j2_isbtag, j3_pt, j3_px, j3_py, j3_eta, j3_m, j3_isbtag, j4_pt, j4_px, j4_py, j4_eta, j4_m, j4_isbtag, j5_pt, j5_px, j5_py, j5_eta, j5_m, j5_isbtag, j6_pt, j6_px, j6_py, j6_eta, j6_m, j6_isbtag, lep_pt, lep_px, lep_py, lep_eta, met_met, met_phi-sin, met_phi-cos
Y Keys: j1_isTruth, j2_isTruth, j3_isTruth, j4_isTruth, j5_isTruth, j6_isTruth
Y Scaled Keys: j1_isTruth, j2_isTruth, j3_isTruth, j4_isTruth, j5_isTruth, j6_isTruth

 ---------------------------------------------------  
Learning Rate: Polynomial Decay with initial_lr=0.01, final_lr=0.0005, decay_step=10000, and power=0.25 
Batch Size: 1000 
Max Number of Epochs: 256 
Number of Epochs Used: 119 
Patience: 8 
Training Time: 00:03:09:44 
Training History: 
     loss      mae      mse  val_loss  val_mae  val_mse
0.281504 0.177602 0.088790  0.269428 0.169704 0.084478
0.267685 0.168108 0.083858  0.266415 0.167502 0.083429
0.264970 0.166227 0.082913  0.264047 0.165100 0.082586
0.262697 0.164613 0.082113  0.262081 0.163570 0.081884
0.260950 0.163366 0.081498  0.260542 0.162103 0.081356
0.259465 0.162301 0.080975  0.259099 0.161912 0.080851
0.258160 0.161367 0.080517  0.258090 0.162116 0.080485
0.257036 0.160553 0.080124  0.256894 0.161081 0.080073
0.256047 0.159842 0.079774  0.255972 0.159853 0.079739
0.255260 0.159285 0.079498  0.255446 0.159878 0.079559
0.254596 0.158800 0.079266  0.254710 0.159006 0.079301
0.254021 0.158390 0.079066  0.254123 0.158611 0.079095
0.253524 0.158033 0.078891  0.253751 0.158144 0.078960
0.253067 0.157700 0.078729  0.253277 0.158074 0.078800
0.252633 0.157379 0.078576  0.252839 0.157020 0.078641
0.252243 0.157102 0.078438  0.252594 0.156779 0.078555
0.251895 0.156850 0.078316  0.252027 0.156201 0.078364
0.251582 0.156619 0.078208  0.251937 0.156080 0.078325
0.251305 0.156425 0.078109  0.251659 0.157429 0.078238
0.251045 0.156233 0.078019  0.251451 0.155618 0.078163
0.250803 0.156058 0.077933  0.251041 0.155977 0.078017
0.250543 0.155874 0.077841  0.251063 0.155709 0.078026
0.250304 0.155695 0.077756  0.250778 0.155403 0.077900
0.250073 0.155529 0.077674  0.250477 0.156481 0.077842
0.249837 0.155365 0.077591  0.249958 0.155346 0.077629
0.249585 0.155177 0.077499  0.249880 0.155122 0.077591
0.249342 0.154995 0.077411  0.249582 0.154729 0.077498
0.249110 0.154834 0.077328  0.249384 0.155046 0.077418
0.248861 0.154646 0.077236  0.249115 0.154405 0.077312
0.248576 0.154440 0.077131  0.248747 0.154494 0.077191
0.248196 0.154153 0.076988  0.248626 0.154448 0.077109
0.247858 0.153916 0.076861  0.247938 0.153937 0.076901
0.247608 0.153736 0.076772  0.248225 0.153930 0.077002
0.247396 0.153589 0.076697  0.247678 0.152800 0.076781
0.247226 0.153462 0.076637  0.247464 0.153383 0.076714
0.247059 0.153341 0.076575  0.247441 0.152977 0.076698
0.246892 0.153220 0.076516  0.247472 0.154161 0.076717
0.246758 0.153131 0.076470  0.247068 0.153042 0.076578
0.246607 0.153022 0.076419  0.247023 0.153715 0.076565
0.246472 0.152920 0.076368  0.246859 0.154196 0.076503
0.246341 0.152832 0.076323  0.246665 0.153049 0.076430
0.246225 0.152750 0.076284  0.246503 0.152847 0.076384
0.246123 0.152678 0.076247  0.246635 0.153136 0.076428
0.246017 0.152602 0.076210  0.246349 0.152561 0.076338
0.245921 0.152530 0.076175  0.246150 0.152685 0.076259
0.245838 0.152473 0.076145  0.246241 0.153292 0.076280
0.245740 0.152405 0.076111  0.246058 0.151924 0.076215
0.245637 0.152330 0.076075  0.246069 0.152334 0.076221
0.245538 0.152261 0.076038  0.245854 0.152833 0.076148
0.245443 0.152192 0.076005  0.245896 0.151917 0.076150
0.245339 0.152113 0.075966  0.245646 0.151789 0.076055
0.245216 0.152027 0.075920  0.245724 0.151707 0.076059
0.245083 0.151922 0.075868  0.245621 0.152042 0.076048
0.244968 0.151843 0.075827  0.245442 0.152500 0.075982
0.244856 0.151757 0.075786  0.245160 0.152213 0.075896
0.244774 0.151699 0.075756  0.245160 0.152270 0.075901
0.244672 0.151615 0.075717  0.245038 0.151995 0.075831
0.244569 0.151543 0.075680  0.245371 0.151486 0.075953
0.244482 0.151476 0.075648  0.244878 0.152139 0.075792
0.244392 0.151411 0.075614  0.244795 0.152374 0.075741
0.244311 0.151357 0.075586  0.244712 0.152225 0.075719
0.244242 0.151294 0.075560  0.244589 0.151763 0.075685
0.244183 0.151264 0.075538  0.244775 0.151930 0.075747
0.244116 0.151205 0.075515  0.244571 0.152314 0.075671
0.244071 0.151172 0.075497  0.244652 0.150794 0.075714
0.244018 0.151129 0.075478  0.244566 0.151098 0.075683
0.243963 0.151099 0.075458  0.244366 0.151241 0.075594
0.243913 0.151065 0.075440  0.244743 0.150968 0.075706
0.243871 0.151028 0.075426  0.244231 0.151730 0.075544
0.243814 0.150985 0.075407  0.244375 0.151381 0.075599
0.243739 0.150935 0.075379  0.244159 0.151400 0.075528
0.243674 0.150889 0.075358  0.244329 0.151882 0.075568
0.243621 0.150857 0.075340  0.244257 0.149995 0.075545
0.243562 0.150817 0.075320  0.244030 0.151818 0.075501
0.243524 0.150787 0.075305  0.244038 0.150932 0.075504
0.243470 0.150756 0.075287  0.243939 0.151533 0.075450
0.243429 0.150720 0.075274  0.243866 0.150815 0.075423
0.243385 0.150691 0.075258  0.243689 0.150795 0.075370
0.243347 0.150664 0.075245  0.244055 0.151926 0.075501
0.243308 0.150642 0.075231  0.243574 0.150370 0.075323
0.243275 0.150612 0.075220  0.243847 0.151951 0.075412
0.243244 0.150592 0.075208  0.243767 0.150243 0.075385
0.243210 0.150567 0.075198  0.244048 0.150469 0.075489
0.243179 0.150547 0.075187  0.243715 0.150289 0.075365
0.243135 0.150515 0.075172  0.243650 0.150342 0.075341
0.243097 0.150491 0.075159  0.243574 0.151529 0.075323
0.243071 0.150472 0.075150  0.243547 0.151207 0.075306
0.243039 0.150445 0.075137  0.243544 0.152064 0.075322
0.243011 0.150424 0.075129  0.243430 0.149951 0.075267
0.242997 0.150413 0.075123  0.243685 0.151117 0.075365
0.242968 0.150394 0.075113  0.243543 0.149833 0.075308
0.242941 0.150375 0.075103  0.243485 0.150096 0.075297
0.242929 0.150368 0.075098  0.243352 0.150480 0.075241
0.242896 0.150342 0.075089  0.243632 0.150988 0.075330
0.242881 0.150334 0.075082  0.243305 0.149997 0.075222
0.242856 0.150311 0.075075  0.243350 0.150516 0.075235
0.242827 0.150294 0.075064  0.243377 0.150592 0.075251
0.242804 0.150274 0.075054  0.243223 0.150391 0.075205
0.242778 0.150254 0.075045  0.243421 0.150123 0.075256
0.242754 0.150238 0.075038  0.243677 0.151426 0.075360
0.242723 0.150214 0.075025  0.243342 0.149278 0.075211
0.242700 0.150197 0.075017  0.243098 0.149867 0.075159
0.242677 0.150178 0.075008  0.243360 0.149758 0.075229
0.242657 0.150162 0.075002  0.243082 0.149977 0.075147
0.242626 0.150142 0.074991  0.243097 0.151193 0.075155
0.242612 0.150137 0.074986  0.243189 0.150373 0.075193
0.242594 0.150117 0.074980  0.243408 0.150613 0.075249
0.242573 0.150105 0.074973  0.243171 0.150054 0.075195
0.242548 0.150090 0.074964  0.243004 0.149801 0.075127
0.242542 0.150078 0.074961  0.243220 0.150106 0.075193
0.242520 0.150065 0.074954  0.242925 0.150761 0.075089
0.242511 0.150060 0.074951  0.243144 0.150213 0.075171
0.242488 0.150041 0.074942  0.243062 0.150793 0.075133
0.242482 0.150034 0.074939  0.242937 0.150247 0.075095
0.242468 0.150030 0.074936  0.243155 0.150429 0.075180
0.242457 0.150015 0.074930  0.243027 0.149879 0.075138
0.242435 0.150003 0.074922  0.243015 0.150517 0.075124
0.242423 0.149997 0.074920  0.243077 0.150214 0.075153
0.242409 0.149983 0.074914  0.243028 0.149670 0.075115 

 ---------------------------------------------------  
Model Architecture:
Model: "model"
_____________________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     Trainable  
=============================================================================================================
 jet_input (InputLayer)         [(None, 6, 6)]       0           []                               Y          
                                                                                                             
 other_input (InputLayer)       [(None, 7)]          0           []                               Y          
                                                                                                             
 flattened_jets (Flatten)       (None, 36)           0           ['jet_input[0][0]']              Y          
                                                                                                             
 concat_jets_other (Concatenate  (None, 43)          0           ['other_input[0][0]',            Y          
 )                                                                'flattened_jets[0][0]']                    
                                                                                                             
 dense256_1 (Dense)             (None, 256)          11264       ['concat_jets_other[0][0]']      Y          
                                                                                                             
 dense256_2 (Dense)             (None, 256)          65792       ['dense256_1[0][0]']             Y          
                                                                                                             
 jet_match_output (Dense)       (None, 6)            1542        ['dense256_2[0][0]']             Y          
                                                                                                             
=============================================================================================================
Total params: 78,598
Trainable params: 78,598
Non-trainable params: 0
_____________________________________________________________________________________________________________
